from bs4 import BeautifulSoup
import re
from typing import List, Dict, Any
from utils.table_classifier import classify_table  # å¯¼å…¥åˆ†ç±»å™¨
from config import HEADER_INDICATORS, HEADER_KEYWORDS


class HtmlTableParser:
    """HTMLè¡¨æ ¼è§£æå™¨"""
    
    def __init__(self, html: str):
        self.raw_html = html
        self.html = ""
        self.tables = []
        self.segments = []

    # ======================================================
    # Step 1ï¼šæ¸…æ´— HTML
    # ======================================================
    def clean_html(self) -> str:
        """æ¸…æ´—HTMLå†…å®¹"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦æˆªæ–­HTML
        self.raw_html = self._truncate_html_if_needed(self.raw_html)
        
        soup = BeautifulSoup(self.raw_html, "html.parser")

        # åˆ é™¤æ— å…³æ ‡ç­¾
        for tag in soup(["script", "style", "meta", "link"]):
            tag.decompose()

        # å»æ‰å¤šä½™ç©ºç™½
        self.html = str(soup).replace('<br/>', '/').replace('\r\n', ' ').replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        self.html = re.sub(r'\s+', ' ', self.html).strip()

        return self.html



    def _truncate_html_if_needed(self, html: str) -> str:
        """
        é€šè¿‡é‚®ä»¶å¤´ç‰¹å¾åˆ¤æ–­ç›–æ¥¼ï¼š
        - å‘ä»¶äººè¡ŒåŒ…å«é‚®ç®± <...@...>
        - ç¬¬äºŒå±‚æ¥¼å‡ºç°ç¬¬äºŒä¸ªç±»ä¼¼çš„å‘ä»¶äººè¡Œå°±æˆªæ–­
        """

        # åŒ¹é… 'å‘ä»¶äºº' åé¢è·Ÿ <é‚®ç®±> çš„æ¨¡å¼
        header_patterns = [
            r'(?:å‘ä»¶äºº|From|Sender|from)[\s\S]{0,500}?(?:æ—¶é—´|Date|æ—¥æœŸ|å‘é€æ—¶é—´|å‘é€æ—¥æœŸ|time)',
            r'(?:From|å‘ä»¶äºº|Sender)[\s\S]{0,500}?(?:Date|æ—¶é—´|æ—¥æœŸ|å‘é€æ—¶é—´|å‘é€æ—¥æœŸ|time)',
            r'(?:å‘ä»¶äºº|From|Sender)[\s\S]{0,500}?(?:æ”¶ä»¶äºº|To|æ”¶ä¿¡äºº)',
            r'(?:From|å‘ä»¶äºº|Sender)[\s\S]{0,500}?(?:ä¸»é¢˜|Subject|subject)',
            r'(?:å‘ä»¶äºº|From|Sender)[\s\S]{0,500}?(?:é‚®ç®±|email|<.*@.*>)',
            r'(?:å‘ä»¶äºº|From|Sender)[\s\S]{0,500}?(?:\d{4}-\d{2}-\d{2}|\d{2}/\d{2}/\d{4})',  # æ—¥æœŸæ ¼å¼
        ]

        matches = []
        for pattern in header_patterns:
            pattern_matches = list(re.finditer(pattern, html))
            matches.extend(pattern_matches)
        
        # æŒ‰ä½ç½®æ’åºå¹¶å»é‡
        matches = sorted(matches, key=lambda x: x.start())
        
        # å»é‡ï¼šä¿ç•™ä½ç½®ç›¸è¿‘çš„åŒ¹é…ä¸­çš„ç¬¬ä¸€ä¸ª
        unique_matches = []
        last_end = -1
        for match in matches:
            if match.start() > last_end:  # é¿å…é‡å åŒ¹é…
                unique_matches.append(match)
                last_end = match.end()
        
        # åªæœ‰ 0 æˆ– 1 ä¸ªå®Œæ•´é‚®ä»¶å¤´ï¼Œä¸æˆªæ–­
        if len(unique_matches) <= 1:
            return html

        # ç¬¬ä¸€å±‚æ¥¼ä»ç¬¬ä¸€ä¸ªåŒ¹é…å¼€å§‹
        first_start = unique_matches[0].start()

        # æˆªæ–­ç‚¹ä¸ºç¬¬äºŒä¸ªåŒ¹é…å¼€å§‹
        second_start = unique_matches[1].start()

        return html[first_start:second_start]


    # åˆ¤æ–­æ˜¯å¦ä¸ºå­—æ®µè¡Œ
    def _is_header_row(self, row: List[str]) -> bool:
        """
        åˆ¤æ–­ä¸€è¡Œæ˜¯å¦ä¸ºè¡¨å¤´è¡Œ
        
        Args:
            row: è¡¨æ ¼è¡Œæ•°æ®
            
        Returns:
            bool: æ˜¯å¦ä¸ºè¡¨å¤´è¡Œ
        """
        # å¦‚æœè¡Œä¸ºç©ºæˆ–åªæœ‰ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä¸æ˜¯è¡¨å¤´
        if not row or all(not cell.strip() for cell in row):
            return False
        
        # ç»Ÿè®¡éç©ºå•å…ƒæ ¼æ•°é‡
        non_empty_cells = [cell for cell in row if cell.strip()]
        
        # å¦‚æœéç©ºå•å…ƒæ ¼å¾ˆå°‘ï¼Œå¯èƒ½æ˜¯è¡¨å¤´
        if len(non_empty_cells) <= 2:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…¸å‹çš„è¡¨å¤´å…³é”®è¯
        header_keywords = HEADER_KEYWORDS
        header_text = ' '.join(non_empty_cells).lower()
        
        # å¦‚æœåŒ…å«è¡¨å¤´å…³é”®è¯ï¼Œåˆ™è®¤ä¸ºæ˜¯è¡¨å¤´
        for keyword in header_keywords:
            if keyword in header_text:
                return True
        
        # æ£€æŸ¥æ˜¯å¦å¤§éƒ¨åˆ†å•å…ƒæ ¼éƒ½æ˜¯ç®€çŸ­çš„æ–‡å­—ï¼ˆé€šå¸¸æ˜¯è¡¨å¤´ç‰¹å¾ï¼‰
        short_text_cells = [cell for cell in non_empty_cells if len(cell.strip()) <= 15]
        if len(short_text_cells) / len(non_empty_cells) >= 0.7:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„è¡¨å¤´æ ‡è¯†ç¬¦
        header_indicators = HEADER_INDICATORS
        for indicator in header_indicators:
            if indicator in header_text.upper():
                return True
        
        return False




    # ======================================================
    # Step 2ï¼šæå–æ‰€æœ‰ table åŸºæœ¬ä¿¡æ¯
    # ======================================================
    def extract_tables(self) -> List[Dict]:
        """æå–æ‰€æœ‰è¡¨æ ¼åŸºæœ¬ä¿¡æ¯"""
        table_pattern = r"<table[^>]*>.*?</table>"
        table_htmls = re.findall(table_pattern, self.html, re.DOTALL | re.IGNORECASE)

        tables = []
        positions = []  # è®°å½•æ¯ä¸ªè¡¨æ ¼åœ¨HTMLä¸­çš„ä½ç½®
        
        for idx, table_html in enumerate(table_htmls):
            start_pos = self.html.find(table_html)
            end_pos = start_pos + len(table_html)
            
            soup = BeautifulSoup(table_html, "html.parser")
            tbody = soup.find("tbody")
            tr_count = len(tbody.find_all("tr")) if tbody else 0

            tables.append({
                "index": idx,
                "html": table_html,
                "tr_count": tr_count,
                "is_single_row": tr_count == 1,
                "start_pos": start_pos,
                "end_pos": end_pos
            })
            
            positions.append((start_pos, end_pos))

        self.tables = tables
        return tables

    # ======================================================
    # Step 3ï¼šè§£æéè¡¨æ ¼å†…å®¹å’Œè¡¨æ ¼å†…å®¹ä¸ºç»Ÿä¸€åˆ—è¡¨
    # ======================================================
    def parse_to_section_list(self) -> List[Dict]:
        """å°†HTMLå†…å®¹è§£æä¸ºç»Ÿä¸€çš„æ®µè½åˆ—è¡¨ï¼ŒåŒ…å«æ–‡æœ¬å’Œè¡¨æ ¼"""
        # æå–æ‰€æœ‰è¡¨æ ¼ä¿¡æ¯
        self.extract_tables()
        
        # è·å–æ‰€æœ‰å†…å®¹å—ï¼ˆæ–‡æœ¬å’Œè¡¨æ ¼ï¼‰çš„ä½ç½®ä¿¡æ¯
        content_blocks = []
        
        # æ·»åŠ æ–‡æœ¬å—
        last_end = 0
        for table in self.tables:
            start_pos = table["start_pos"]
            end_pos = table["end_pos"]
            
            # æ·»åŠ è¡¨æ ¼å‰é¢çš„æ–‡æœ¬å†…å®¹
            if start_pos > last_end:
                text_content = self.html[last_end:start_pos]
                if text_content.strip():
                    # å°†æ–‡æœ¬æŒ‰HTMLç»“æ„åˆ†å‰²
                    blocks = self._split_text_by_html_structure(text_content)
                    for block in blocks:
                        content_blocks.append({
                            "type": "text",
                            "content": block,
                            "position": (last_end, start_pos)
                        })
            
            # æ·»åŠ è¡¨æ ¼å†…å®¹
            content_blocks.append({
                "type": "table",
                "table_info": table,
                "position": (start_pos, end_pos)
            })
            
            last_end = end_pos
        
        # æ·»åŠ æœ€åçš„æ–‡æœ¬å†…å®¹
        if last_end < len(self.html):
            text_content = self.html[last_end:]
            if text_content.strip():
                # å°†æ–‡æœ¬æŒ‰HTMLç»“æ„åˆ†å‰²
                blocks = self._split_text_by_html_structure(text_content)
                for block in blocks:
                    content_blocks.append({
                        "type": "text",
                        "content": block,
                        "position": (last_end, len(self.html))
                    })
        
        # æŒ‰ä½ç½®æ’åº
        content_blocks.sort(key=lambda x: x["position"][0])
        
        # å»é‡å¤„ç†
        content_blocks = self._remove_duplicate_blocks(content_blocks)
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        result = []
        
        for block in content_blocks:
            if block["type"] == "text":
                # æ–‡æœ¬å—
                content_text = self._extract_text_from_html(block["content"])
                # è¿‡æ»¤æ‰ç©ºå†…å®¹æˆ–æ— æ„ä¹‰å†…å®¹
                if self._is_meaningful_content(content_text):
                    result.append({
                        "section_type": "text",
                        "table_type": "",  # æ–‡æœ¬å—æ­¤é¡¹ä¸ºç©º
                        "upcontent": "",  # è¡¨æ ¼ä¸Šæ–¹å†…å®¹
                        "content": content_text,  # æ ¸å¿ƒå†…å®¹åœ¨æ­¤
                        "bottom_content": "",  # è¡¨æ ¼ä¸‹æ–¹å†…å®¹
                        "cols": [],  # æ–‡æœ¬å—æ­¤é¡¹ä¸ºç©ºæ•°ç»„
                        "rows": []  # æ–‡æœ¬å—æ­¤é¡¹ä¸ºç©ºæ•°ç»„
                    })
            else:
                # è¡¨æ ¼å—
                table_info = block["table_info"]
                data = self.parse_table_cells(table_info["html"])
                
                if not data:
                    continue
                
                # æ ¹æ®è¡¨å¤´è¡Œåˆ†å‰²è¡¨æ ¼æ•°æ®
                sections = self._split_table_by_headers(data)
                print('sections===========>>>>>>', sections)
                
                # ä¸ºæ¯ä¸ªsectionåˆ›å»ºä¸€ä¸ªè¡¨æ ¼å¯¹è±¡
                for i, section in enumerate(sections):
                    table_obj = {
                        "section_type": "table",
                        "table_type": "",  # ç¨åå¡«å……
                        "upcontent": "",  # è¡¨æ ¼ä¸Šæ–¹å†…å®¹
                        "content": "",  # æ–‡æœ¬å—çš„å†…å®¹ï¼Œè¡¨æ ¼å—é€šå¸¸ä¸ºç©º
                        "bottom_content": "",  # è¡¨æ ¼ä¸‹æ–¹å†…å®¹
                        "cols": section["headers"],  # è¡¨å¤´è¡Œ
                        "rows": section["data"],  # æ•°æ®è¡Œ
                        "table_id": f"table_{table_info['index']}_{i}"  # æ·»åŠ table_id
                    }
                    
                    # å¯¹è¡¨æ ¼è¿›è¡Œåˆ†ç±»
                    table_type = classify_table({
                        "col": table_obj["cols"],
                        "data": table_obj["rows"]
                    })
                    table_obj["table_type"] = table_type.value  # æ·»åŠ åˆ†ç±»ä¿¡æ¯
                    
                    result.append(table_obj)
        
        return result


    
    def _split_table_by_headers(self, rows: List[List[str]]) -> List[Dict]:
        """
        æ ¹æ®è¡¨å¤´è¡Œåˆ†å‰²è¡¨æ ¼æ•°æ®
        
        Args:
            rows: è¡¨æ ¼çš„æ‰€æœ‰è¡Œ
            
        Returns:
            List[Dict]: åŒ…å«headerså’Œdataçš„sectionåˆ—è¡¨
        """
        if not rows:
            return []
        
        sections = []
        current_headers = []
        current_data = []
        
        for row in rows:
            if self._is_header_row(row):
                # å¦‚æœå½“å‰å·²ç»æœ‰è¡¨å¤´å’Œæ•°æ®ï¼Œä¿å­˜ä¸ºä¸€ä¸ªsection
                if current_headers or current_data:
                    sections.append({
                        "headers": current_headers,
                        "data": current_data
                    })
                # å¼€å§‹æ–°çš„section
                current_headers = [row]  # æ–°çš„è¡¨å¤´è¡Œ
                current_data = []  # é‡ç½®æ•°æ®è¡Œ
            else:
                # æ·»åŠ æ•°æ®è¡Œåˆ°å½“å‰section
                current_data.append(row)
        
        # ä¿å­˜æœ€åä¸€ä¸ªsection
        if current_headers or current_data:
            sections.append({
                "headers": current_headers,
                "data": current_data
            })
        
        return sections



    def _split_text_by_html_structure(self, text: str) -> List[str]:
        """æ ¹æ®HTMLç»“æ„å°†æ–‡æœ¬åˆ†å‰²æˆå—"""
        # ä½¿ç”¨BeautifulSoupè§£ææ–‡æœ¬
        soup = BeautifulSoup(text, "html.parser")
        
        # æå–ä¸»è¦çš„å—çº§å…ƒç´ 
        blocks = []
        # å…ˆå°è¯•æå–åŒ…å«"è½¬å‘æ¶ˆæ¯"çš„divå…ƒç´ 
        all_divs = soup.find_all('div')
        forward_msg_found = False
        for div in all_divs:
            if 'è½¬å‘æ¶ˆæ¯' in div.get_text() and div.get_text().strip():
                # åªæ·»åŠ ç¬¬ä¸€ä¸ªåŒ…å«è½¬å‘æ¶ˆæ¯çš„div
                if not forward_msg_found:
                    blocks.append(str(div))
                    forward_msg_found = True
                # é¿å…æ·»åŠ é‡å¤çš„è½¬å‘æ¶ˆæ¯
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è½¬å‘æ¶ˆæ¯ç›¸å…³çš„å…ƒç´ ï¼Œå†æŸ¥æ‰¾å…¶ä»–å—çº§å…ƒç´ 
        if not blocks:
            for element in soup.find_all(['div', 'p', 'section', 'article', 'blockquote']):
                if element.get_text().strip():
                    blocks.append(str(element))
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°å—çº§å…ƒç´ ï¼Œåˆ™å°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå—
        if not blocks:
            blocks.append(text)
        # é™åˆ¶å—çš„æ•°é‡ï¼Œé¿å…é‡å¤
        if len(blocks) > 5:
            blocks = blocks[:2]
            
        return blocks

    def _merge_text_blocks_by_structure(self, blocks: List[Dict]) -> List[Dict]:
        """åŸºäºHTMLç»“æ„åˆå¹¶æ–‡æœ¬å—"""
        if not blocks:
            return blocks
            
        merged_blocks = []
        i = 0
        
        while i < len(blocks):
            current_block = blocks[i]
            
            # å¦‚æœæ˜¯è¡¨æ ¼å—ï¼Œç›´æ¥æ·»åŠ 
            if current_block["type"] == "table":
                merged_blocks.append(current_block)
                i += 1
                continue
            
            # å¦‚æœæ˜¯æ–‡æœ¬å—
            if current_block["type"] == "text":
                # ç›´æ¥æ·»åŠ æ–‡æœ¬å—ï¼Œä¸è¿›è¡Œé¢å¤–çš„åˆå¹¶
                merged_blocks.append(current_block)
                i += 1
            else:
                i += 1
                
        return merged_blocks

    def _extract_text_from_html(self, html_content: str) -> str:
        """ä»HTMLå†…å®¹ä¸­æå–çº¯æ–‡æœ¬"""
        soup = BeautifulSoup(html_content, "html.parser")
        # è·å–æ–‡æœ¬å†…å®¹å¹¶æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = soup.get_text()
        # å°†å¤šä¸ªè¿ç»­çš„ç©ºç™½å­—ç¬¦æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def _split_text_into_blocks(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬æŒ‰å—åˆ†å‰²ï¼Œä¿ç•™æœ‰æ„ä¹‰çš„æ–‡æœ¬å—"""
        # æŒ‰ç…§æ®µè½æ ‡è®°åˆ†å‰²æ–‡æœ¬
        blocks = re.split(r'(<p[^>]*>.*?</p>|<div[^>]*>.*?</div>|<br\s*/?>)', text, flags=re.DOTALL | re.IGNORECASE)
        # æ¸…ç†å¹¶è¿‡æ»¤ç©ºå—
        cleaned_blocks = []
        for block in blocks:
            cleaned = block.strip()
            if cleaned:
                cleaned_blocks.append(cleaned)
        return cleaned_blocks

    def _split_text_into_paragraphs(self, text: str) -> List[str]:
        """å°†æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²"""
        # ä½¿ç”¨BeautifulSoupæå–æ–‡æœ¬å—
        soup = BeautifulSoup(text, "html.parser")
        # æå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
        texts = []
        for string in soup.stripped_strings:
            if string:
                texts.append(string)
        
        # åˆå¹¶æ–‡æœ¬ï¼Œç”¨ç©ºæ ¼åˆ†éš”
        combined_text = ' '.join(texts)
        # ä½¿ç”¨å¥å­ç»“æŸç¬¦åˆ†å‰²æ–‡æœ¬
        sentences = re.split(r'[.!?]+', combined_text)
        # æ¸…ç†å¹¶è¿‡æ»¤ç©ºå¥å­
        cleaned_sentences = []
        for sentence in sentences:
            cleaned = sentence.strip()
            if cleaned:
                cleaned_sentences.append(cleaned)
        return cleaned_sentences

    def _is_meaningful_content(self, content: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦æœ‰æ„ä¹‰"""
        if not content:
            return False
        
        # å»é™¤ç©ºç™½å­—ç¬¦åçš„é•¿åº¦æ£€æŸ¥
        stripped_content = content.strip()
        if not stripped_content:
            return False
            
        # è¿‡æ»¤æ‰åªåŒ…å«ç¬¦å·æˆ–éå¸¸çŸ­çš„å†…å®¹
        if len(stripped_content) <= 2 and re.match(r'^[/<>\-_\*\+=\[\]{}|\\]*$', stripped_content):
            return False
            
        # è¿‡æ»¤æ‰çœ‹èµ·æ¥åƒHTMLæ ‡ç­¾ç¢ç‰‡çš„å†…å®¹
        if re.match(r'^[</>]*[a-zA-Z]+[</>]*$', stripped_content):
            return False
            
        return True

    # ======================================================
    # Step 4ï¼šè§£æ table â†’ äºŒç»´æ•°ç»„ï¼ˆrowspan + colspanï¼‰
    # ======================================================
    def parse_table_cells(self, table_html: str) -> List[List[str]]:
        """è§£æè¡¨æ ¼å•å…ƒæ ¼ä¸ºäºŒç»´æ•°ç»„"""
        soup = BeautifulSoup(table_html, "html.parser")
        tbody = soup.find("tbody")
        if not tbody:
            # å¦‚æœæ²¡æœ‰tbodyï¼Œç›´æ¥æŸ¥æ‰¾tableä¸‹çš„æ‰€æœ‰tr
            rows = soup.find_all("tr")
        else:
            # å¦‚æœæœ‰tbodyï¼Œåˆ™æŸ¥æ‰¾tbodyä¸‹çš„tr
            rows = tbody.find_all("tr")
        if not rows:
            return []

        grid = []
        span_map = {}  # (row_idx, col_idx) -> value

        for r_idx, row in enumerate(rows):
            cells = row.find_all(["td", "th"])
            cell_idx = 0
            col_idx = 0
            grid_row = []

            # ğŸ‘‰ æ ¸å¿ƒï¼šåªè¦è¿˜æœ‰ tdï¼Œæˆ–è€…å½“å‰ä½ç½®è¢« rowspan å ç€ï¼Œå°±ç»§ç»­
            while cell_idx < len(cells) or (r_idx, col_idx) in span_map:

                # å…ˆå¤„ç† rowspan å ä½
                if (r_idx, col_idx) in span_map:
                    grid_row.append(span_map[(r_idx, col_idx)])
                    col_idx += 1
                    continue

                # æ­£å¸¸ td/th
                cell = cells[cell_idx]
                cell_idx += 1

                val = cell.get_text().strip()
                # å¤„ç† rowspan å’Œ colspan å±æ€§
                rowspan_attr = cell.get("rowspan")
                colspan_attr = cell.get("colspan")
                
                # å°†å±æ€§å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†å¤„ç†
                rowspan_str = str(rowspan_attr) if rowspan_attr else "1"
                colspan_str = str(colspan_attr) if colspan_attr else "1"
                
                rowspan = int(rowspan_str) if rowspan_str.isdigit() else 1
                colspan = int(colspan_str) if colspan_str.isdigit() else 1

                # è®°å½• rowspan / colspan å ä½
                for rs in range(rowspan):
                    for cs in range(colspan):
                        if rs == 0 and cs == 0:
                            continue
                        span_map[(r_idx + rs, col_idx + cs)] = val

                # å½“å‰è¡Œå†™å…¥ colspan æ¬¡
                for _ in range(colspan):
                    grid_row.append(val)
                    col_idx += 1

            grid.append(grid_row)

        # è¿‡æ»¤æ‰å…¨ç©ºçš„è¡Œ
        filtered_grid = []
        for row in grid:
            # æ£€æŸ¥è¡Œæ˜¯å¦å…¨ä¸ºç©º
            if not all(cell == "" for cell in row):
                filtered_grid.append(row)
        
        return filtered_grid

    # ======================================================
    # æ€»å…¥å£
    # ======================================================
    def parse(self) -> List[Dict]:
        """è§£æHTMLçš„æ€»å…¥å£"""
        self.clean_html()
        return self.parse_to_section_list()

    def _remove_duplicate_blocks(self, blocks: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤çš„å†…å®¹å—"""
        if not blocks:
            return blocks
            
        unique_blocks = []
        seen_contents = set()
        
        for block in blocks:
            # å¯¹äºæ–‡æœ¬å—ï¼ŒåŸºäºå†…å®¹å»é‡
            if block["type"] == "text":
                content = self._extract_text_from_html(block["content"])
                # ç®€åŒ–å†…å®¹ç”¨äºæ¯”è¾ƒï¼ˆå»é™¤ç©ºç™½å­—ç¬¦ï¼‰
                simplified_content = re.sub(r'\s+', '', content)
                if simplified_content and simplified_content not in seen_contents:
                    unique_blocks.append(block)
                    seen_contents.add(simplified_content)
            else:
                # å¯¹äºè¡¨æ ¼å—ï¼Œç›´æ¥æ·»åŠ 
                unique_blocks.append(block)
                
        return unique_blocks